{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f97f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4170f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.listdir('/home/shijin/Downloads/FULL_stack_DS/text_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c10ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocab.nips.txt',\n",
       " 'vocab.nytimes.txt',\n",
       " 'vocab.enron.txt',\n",
       " 'vocab.pubmed.txt',\n",
       " 'vocab.kos.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62121b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "    \n",
    "logging.basicConfig(filename='vocab.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s %(levelname)s -%(message)s',\n",
    "                    datefmt='%Y %m %d %H:%M:%S'\n",
    "                   )\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978c46d",
   "metadata": {},
   "source": [
    "Task1\n",
    "q1 = try to find out a count of each and every word in a respective file return a list of tuple with word and its respective count\n",
    "sample example - [('sudh', 6 ) , ('kumar',3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c488c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def wordCount(*args):\n",
    "    \"\"\"\n",
    "    wordCount(file1, file2, ... filen)\n",
    "    wordCount(*args)\n",
    "\n",
    "    This function take file(s) as argument and return word with occurence count.\n",
    "\n",
    "    :param:\n",
    "        *args: file(s).\n",
    "    :return: ('word', count(word)) : list(tuples)\n",
    "    \"\"\"\n",
    "    words=[]\n",
    "    try: \n",
    "        for file in args:\n",
    "            with open(file,'r',encoding='utf8') as f:\n",
    "                data = csv.reader(f)\n",
    "                for row_data in data:\n",
    "                    words.append(row_data[0])\n",
    "    except Exception as e:\n",
    "        logger.error('error in reading files: ' + str(e))\n",
    "    else:\n",
    "        word_count=[]\n",
    "        unique_words = list(set(words))\n",
    "        unique_words.sort()\n",
    "        for i in unique_words:\n",
    "            word_count.append((i,words.count(i)))\n",
    "        logger.info('Word count for records: ' +  str(len(word_count)) + ' from ' + str(len(args)) + 'Datasets')\n",
    "        \n",
    "        return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a883c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1= wordCount('vocab.enron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f26adfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dbc71",
   "metadata": {},
   "source": [
    "Task 2:\n",
    "q2 = try to perform a reduce operation to get a count of all the word starting with same alphabet\n",
    "sample examle = [(a,56) , (b,34),...........]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e27940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def wordCount_reduce(*args):\n",
    "    \"\"\"\n",
    "    wordCount_reduce(file1, file2, ... filen)\n",
    "    wordCount_reduce(*args)\n",
    "\n",
    "    This function take file(s) as argument and return alphabets with occurence count.\n",
    "\n",
    "    :param:\n",
    "        *args: file(s).\n",
    "    :return: ('alphabets', count(alphabets)) : list(tuples)\n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    try:\n",
    "        for file in args:\n",
    "            with open (file,'r') as f:\n",
    "                data = csv.reader(f)\n",
    "                for row_data in data:\n",
    "                    if row_data[0][0]>='a' and row_data[0][0]<='z' or row_data[0][0]>='A' and row_data[0][0]<='Z':\n",
    "                        words.append(row_data[0][0])\n",
    "    except Exception as e:\n",
    "        logging.error('Error in reading files: ' + str(e))\n",
    "    else:\n",
    "        letter_count = []\n",
    "        unique_letter = list(set(words))\n",
    "        unique_letter.sort()\n",
    "        \n",
    "        for i in unique_letter:\n",
    "            letter_count.append((i,words.count(i)))\n",
    "        \n",
    "        logger.info('Total filter records: ' +  str(len(letter_count)) + ' from ' + str(len(args)) + 'Datasets')\n",
    "        \n",
    "        return letter_count\n",
    "                        \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0434bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2= wordCount_reduce('/home/shijin/Downloads/FULL_stack_DS/text_data/vocab.enron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "814fcfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1800), ('b', 1557), ('c', 2611), ('d', 1664), ('e', 1404)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fa340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1ba554",
   "metadata": {},
   "source": [
    "Task 3:\n",
    "q3 = Try to filter out all the words from dataset .\n",
    ".001.abstract = abstract =.002 = delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "751c237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def filter_word(*args):\n",
    "    \"\"\"\n",
    "    filter_word(file1, file2, ... filen)\n",
    "    filter_word(*args)\n",
    "\n",
    "    This function take file(s) as argument extract only the letters and return  as list.\n",
    "\n",
    "    :param:\n",
    "        *args: file(s).\n",
    "    :return: ('characters') : list(strings)\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    \n",
    "    try:\n",
    "        for file in args:\n",
    "            with open(file, 'r', encoding=\"utf8\") as f:\n",
    "                data = csv.reader(f)\n",
    "                for row_data in data:\n",
    "                    current_word = \"\"\n",
    "                    for char in row_data[0]:\n",
    "                        if char >= 'a' and char <= 'z' or char[0][0] >= 'A' and char[0][0] <= 'Z':\n",
    "                            current_word += char\n",
    "                    words.append(current_word)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error('Error in reading files: ' + str(e))\n",
    "        \n",
    "    else:\n",
    "        alpha = []\n",
    "        unique_words = list(set(words))\n",
    "        unique_words.sort()\n",
    "\n",
    "\n",
    "        for i in unique_words:\n",
    "             #yield (i)\n",
    "            if len(i) > 0:\n",
    "                alpha.append(i)\n",
    "                \n",
    "        logger.info('Total filter records: ' +  str(len(alpha)) + ' from ' + str(len(args)) + 'Datasets')\n",
    "        \n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd48523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 =filter_word('/home/shijin/Downloads/FULL_stack_DS/text_data/vocab.enron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a0cdd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'aaas', 'aactive', 'aadvantage', 'aaker']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b654e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c906d1a7",
   "metadata": {},
   "source": [
    "Task 4:\n",
    "q4 = create a tuple set of all the records avaialble in all the five file and then store it in sqllite DB .\n",
    "(aah,>=,354,fdsf,wer)\n",
    "\n",
    "Function to fetch records from n collecti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62347445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def task4(*args):\n",
    "    \"\"\"\n",
    "    task4(file1, file2, ... filen)\n",
    "    task4(*args)\n",
    "\n",
    "    This function take file(s) as argument and return list of list.\n",
    "\n",
    "    :param:\n",
    "        *args: file(s).\n",
    "    :return: ('string') : list(strings)\n",
    "    \"\"\"\n",
    "    combine_words = []\n",
    "    try:\n",
    "        for file in args:\n",
    "            with open(file, 'r', encoding=\"utf8\") as f:\n",
    "                data = csv.reader(f)\n",
    "                words = []\n",
    "                for row_data in data:\n",
    "                    words.append(row_data[0])\n",
    "                combine_words.append(words)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error('Error in reading files: ' + str(e))\n",
    "    else:\n",
    "        logger.info('Success Return of list of list')\n",
    "                \n",
    "        return combine_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022998ec",
   "metadata": {},
   "source": [
    "fetching data from all 5 collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37648011",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = task4('vocab.enron.txt', 'vocab.kos.txt', 'vocab.nips.txt', 'vocab.nytimes.txt', 'vocab.pubmed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0e5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa39d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into 5 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3242364",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = file[0]\n",
    "f2 = file[1]\n",
    "f3 = file[2]\n",
    "f4 = file[3]\n",
    "f5 = file[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18f36584",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_zip_list = list(zip(f1, f2, f3, f4, f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c48c0fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaa', 'aarp', 'a2i', 'aah', '>='),\n",
       " ('aaas', 'abandon', 'aaa', 'aahed', '>>'),\n",
       " ('aactive', 'abandoned', 'aaai', 'aaron', '>>>'),\n",
       " ('aadvantage', 'abandoning', 'aapo', 'aback', '>/='),\n",
       " ('aaker', 'abb', 'aat', 'abacus', '->')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_zip_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84884e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL da and table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc05ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "try:\n",
    "    db = sqlite3.connect('vocab_database.db')\n",
    "    logger.info('DB created'+str(db))\n",
    "    cursor = db.cursor()\n",
    "    query = 'Create table main_table(file1 text,file2 text,file3 text,file4 text,file5 text) '\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    for record in final_zip_list:\n",
    "        query = 'insert into main_table values {}'.format(tuple(record))\n",
    "        cursor.execute(query)\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    logger.error('Error'+ str(e))\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "371582b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect('vocab_database.db')\n",
    "cursor= db.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "186d64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.execute('select * from main_table limit 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7329b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaa', 'aarp', 'a2i', 'aah', '>='),\n",
       " ('aaas', 'abandon', 'aaa', 'aahed', '>>'),\n",
       " ('aactive', 'abandoned', 'aaai', 'aaron', '>>>'),\n",
       " ('aadvantage', 'abandoning', 'aapo', 'aback', '>/='),\n",
       " ('aaker', 'abb', 'aat', 'abacus', '->'),\n",
       " ('aap', 'abc', 'aazhang', 'abajo', '--'),\n",
       " ('aapg', 'abcs', 'abandonment', 'abalone', '-->'),\n",
       " ('aaron', 'abdullah', 'abbott', 'abandon', '-/-'),\n",
       " ('aarp', 'ability', 'abbreviated', 'abandoned', '-/+'),\n",
       " ('aas', 'aboard', 'abcde', 'abandoning', '/-'),\n",
       " ('aau', 'abortion', 'abe', 'abandonment', '/+-'),\n",
       " ('ab1890', 'abortions', 'abeles', 'abandono', '..'),\n",
       " ('ab1x', 'abraham', 'abi', 'abarnard', '...'),\n",
       " ('ab31x', 'abrams', 'abilistic', 'abashed', '+-'),\n",
       " ('aba', 'abroad', 'abilities', 'abate', '+/'),\n",
       " ('abacus', 'absence', 'ability', 'abated', '+/--'),\n",
       " ('abag', 'absent', 'abl', 'abatement', '+/?'),\n",
       " ('abalone', 'absentee', 'able', 'abating', '+/+'),\n",
       " ('abandon', 'absolute', 'ables', 'abbey', '++'),\n",
       " ('abandoned', 'absolutely', 'ablex', 'abbot', '+++')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e69a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
